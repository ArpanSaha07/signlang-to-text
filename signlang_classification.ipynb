{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpanSaha07/signlang-to-text/blob/main/signlang_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the start of our sign language detection project. Authors: Emilia and Arpan"
      ],
      "metadata": {
        "id": "VtG_zWEMR7D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DA3ezRzISSbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/signlang_detect/' \n",
        "\n",
        "# change the runtime type to GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2bWeikjjQGp",
        "outputId": "065a1c66-c21a-4cf4-8821-e56771e669d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Loading the data"
      ],
      "metadata": {
        "id": "fj7bu38OSzhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_alphabet = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','nothing','del','space']\n",
        "labels_numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
        "img_size = 500\n",
        "\n",
        "def get_data(data_dir):\n",
        "  data = []\n",
        "  for label in labels_numbers: \n",
        "    path = os.path.join(data_dir, label)\n",
        "    class_num = labels_numbers.index(label)\n",
        "    for img in os.listdir(path):\n",
        "      try:\n",
        "        img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
        "        resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "        data.append([resized_arr, class_num])\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "  return np.array(data)\n",
        "  \n",
        "train_numbers = get_data('../rootpath/train')  #need to change address\n",
        "val_numbers = get_data('../rootpath/')\n",
        "\n"
      ],
      "metadata": {
        "id": "k_CWOc-DVDJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Visualize the data"
      ],
      "metadata": {
        "id": "gGqOLYC3bR3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This block of code helps us visualize our dataset and see how many images of every label do we have\n",
        "occurance = []\n",
        "for img_number in train_numbers:\n",
        "  occurance.append(img_number[1]) #this append the label of the image\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(occurance)\n",
        "\n",
        "#This is just fun because we can visualize images from the dataset if we need\n",
        "plt.figure(figsize = (5,5))\n",
        "plt.imshow(train_numbers[1][0]) #train_numbers[1][0] gives the jpeg file\n",
        "plt.title(labels_numbers[train_numbers[0][1]]) #train_numbers[0][1] gives the label of the image"
      ],
      "metadata": {
        "id": "wWyxR8-qbWGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Data augmentation"
      ],
      "metadata": {
        "id": "ANpvdcEFbasr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_nb_train = []\n",
        "y_nb_train = []\n",
        "x_nb_val = []\n",
        "y_nb_val = []\n",
        "\n",
        "for feature, label in train_numbers:\n",
        "  x_nb_train.append(feature)\n",
        "  y_nb_train.append(label)\n",
        "\n",
        "for feature, label in val_numbers:\n",
        "  x_nb_val.append(feature)\n",
        "  y_nb_val.append(label)\n",
        "\n",
        "# Normalize the data\n",
        "x_nb_train = np.array(x_nb_train) / 255\n",
        "x_nb_val = np.array(x_nb_val) / 255\n",
        "\n",
        "x_nb_train.reshape(-1, img_size, img_size, 1)\n",
        "y_train = np.array(y_nb_train)\n",
        "\n",
        "x_nb_val.reshape(-1, img_size, img_size, 1)\n",
        "y_nb_val = np.array(y_nb_val)\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_nb_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "fvBYgSxUbdbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Defining our model"
      ],
      "metadata": {
        "id": "8I5Arftmgdvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(img_size,img_size,3)))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation=\"relu\"))\n",
        "model.add(Dense(2, activation=\"softmax\")) \n",
        "\n",
        "opt = Adam(lr=0.000001)\n",
        "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(x_nb_train,y_nb_train,epochs = 500 , validation_data = (x_nb_val, y_nb_val))\n"
      ],
      "metadata": {
        "id": "y3reQrPDgjRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Visualizing our results"
      ],
      "metadata": {
        "id": "smt-TAZeoJZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(500)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-4ipddFloN25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}